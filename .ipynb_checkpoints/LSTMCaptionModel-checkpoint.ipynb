{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836b6d14",
   "metadata": {},
   "source": [
    "# Captioning Model\n",
    "\n",
    "This notebook will contain the code to train the LSTM captioning models (decoder). We load the swin-t encoder that was pre-trained on the 18-attribute data from the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df90d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import utils.load_funcs\n",
    "import json\n",
    "import torch,torchvision\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e047de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 329, 224])\n",
      "torch.Size([64, 18])\n",
      "tensor([[15,  2, 49,  ...,  0,  0,  0],\n",
      "        [15, 53, 49,  ...,  0,  0,  0],\n",
      "        [15,  2, 82,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [15,  2, 33,  ...,  0,  0,  0],\n",
      "        [15,  2, 64,  ...,  0,  0,  0],\n",
      "        [15,  2, 33,  ...,  0,  0,  0]]) torch.Size([64, 109])\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_loader, val_loader = utils.load_funcs.get_data_loaders()\n",
    "images, labels, captions = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(captions, captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9300ff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000080-01_7_additional.jpg</th>\n",
       "      <td>The lower clothing is of long length. The fabr...</td>\n",
       "      <td>[15, 2, 78, 44, 3, 34, 35, 54, 1, 2, 5, 3, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-01_7_additional.jpg</th>\n",
       "      <td>His tank top has sleeves cut off, cotton fabri...</td>\n",
       "      <td>[15, 53, 20, 45, 12, 30, 67, 68, 25, 10, 5, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-02_7_additional.jpg</th>\n",
       "      <td>His sweater has long sleeves, cotton fabric an...</td>\n",
       "      <td>[15, 53, 49, 12, 35, 30, 25, 10, 5, 8, 84, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-03_7_additional.jpg</th>\n",
       "      <td>His shirt has short sleeves, cotton fabric and...</td>\n",
       "      <td>[15, 53, 17, 12, 51, 30, 25, 10, 5, 8, 28, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-04_7_additional.jpg</th>\n",
       "      <td>The sweater the person wears has long sleeves,...</td>\n",
       "      <td>[15, 2, 49, 2, 26, 9, 12, 35, 30, 25, 47, 5, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           0  \\\n",
       "MEN-Denim-id_00000080-01_7_additional.jpg  The lower clothing is of long length. The fabr...   \n",
       "MEN-Denim-id_00000089-01_7_additional.jpg  His tank top has sleeves cut off, cotton fabri...   \n",
       "MEN-Denim-id_00000089-02_7_additional.jpg  His sweater has long sleeves, cotton fabric an...   \n",
       "MEN-Denim-id_00000089-03_7_additional.jpg  His shirt has short sleeves, cotton fabric and...   \n",
       "MEN-Denim-id_00000089-04_7_additional.jpg  The sweater the person wears has long sleeves,...   \n",
       "\n",
       "                                                                                    sequence  \n",
       "MEN-Denim-id_00000080-01_7_additional.jpg  [15, 2, 78, 44, 3, 34, 35, 54, 1, 2, 5, 3, 10,...  \n",
       "MEN-Denim-id_00000089-01_7_additional.jpg  [15, 53, 20, 45, 12, 30, 67, 68, 25, 10, 5, 8,...  \n",
       "MEN-Denim-id_00000089-02_7_additional.jpg  [15, 53, 49, 12, 35, 30, 25, 10, 5, 8, 84, 7, ...  \n",
       "MEN-Denim-id_00000089-03_7_additional.jpg  [15, 53, 17, 12, 51, 30, 25, 10, 5, 8, 28, 14,...  \n",
       "MEN-Denim-id_00000089-04_7_additional.jpg  [15, 2, 49, 2, 26, 9, 12, 35, 30, 25, 47, 5, 3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaea9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk': 0, '.': 1, 'the': 2, 'is': 3, 'a': 4, 'fabric': 5, 'with': 6, 'patterns': 7, 'and': 8, 'wears': 9, 'cotton': 10, 'her': 11, 'has': 12, 'this': 13, 'color': 14, 'sos': 15, 'eos': 16, 'shirt': 17, 'on': 18, 'there': 19, 'tank': 20, 'it': 21, 'neckline': 22, 'ring': 23, 'an': 24, ',': 25, 'person': 26, 'accessory': 27, 'pure': 28, 'solid': 29, 'sleeves': 30, 'wearing': 31, 'lady': 32, 'female': 33, 'of': 34, 'long': 35, 'pants': 36, 'are': 37, 'three': 38, 'finger': 39, 'wrist': 40, 'graphic': 41, 'point': 42, 'shorts': 43, 'clothing': 44, 'top': 45, 'sleeve': 46, 'its': 47, 'woman': 48, 'sweater': 49, 'denim': 50, 'short': 51, 'in': 52, 'his': 53, 'length': 54, 'crew': 55, 'round': 56, 'chiffon': 57, 't': 58, 'neck': 59, 'neckwear': 60, 'trousers': 61, 'outer': 62, 'hat': 63, 'upper': 64, 'no': 65, 'sleeveless': 66, 'cut': 67, 'off': 68, 'knitting': 69, 'suspenders': 70, 'pattern': 71, 'lapel': 72, 'floral': 73, 'medium': 74, 'v': 75, 'shape': 76, 'head': 77, 'lower': 78, 'other': 79, 'also': 80, 'guy': 81, 'gentleman': 82, 'striped': 83, 'stripe': 84, 'leather': 85, 'belt': 86, 'skirt': 87, 'block': 88, 'complicated': 89, 'mixed': 90, 'leggings': 91, 'waist': 92, 'socks': 93, 'stand': 94, 'man': 95, 'pair': 96, 'plaid': 97, 'lattice': 98, 'sunglasses': 99, 'quarter': 100, 'shoes': 101, 'furry': 102, 'square': 103, 'glasses': 104, 'hands': 105, 'or': 106, 'clothes': 107, 'eyeglasses': 108}\n",
      "Vocab Length:  109\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer and vocab dictionary\n",
    "tokenizer = train_loader.dataset.tokenizer\n",
    "vocab = json.loads(tokenizer.get_config()['index_word'])\n",
    "vocab = {v: int(k)-1 for k, v in vocab.items()}\n",
    "print(vocab)\n",
    "print('Vocab Length: ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fd446",
   "metadata": {},
   "source": [
    "We define our LSTM Decoder and Caption Model classes. The LSTM Decoder we choose has an embedding layer of size 512 with 3 LSTM layers, and each LSTM layer has a hidden dimension of 512. Our feature extraction backbone (encoder) has the classifier head removed, which results in an output feature map of size (1x768)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d65e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Classes for Encoder (Classifier)/Decoder\n",
    "class AttributeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.forks = torch.nn.ModuleList()\n",
    "        for class_count in attribute_classes:\n",
    "            fork = torch.nn.Linear(in_features=in_features, out_features=class_count)\n",
    "            self.forks.append(fork)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for index,fork in enumerate(self.forks):\n",
    "            out_fork = fork(x) #Classification\n",
    "            out.append(out_fork)\n",
    "        return out\n",
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, backbone, backbone_out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = AttributeClassifier(backbone_out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# Define LSTM Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, feature_size, hidden_size, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "        \n",
    "        # lstm cells\n",
    "        self.lstm_cell_1 = nn.LSTMCell(input_size=embed_size+feature_size, hidden_size=hidden_size)\n",
    "        self.lstm_cell_2 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.lstm_cell_3 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    \n",
    "    def forward(self, features, captions, mode='train'):\n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        features = torch.unsqueeze(features, dim=1)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state_1 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_1 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        hidden_state_2 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_2 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        hidden_state_3 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_3 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        max_caption_length = 109\n",
    "        \n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.zeros((batch_size, max_caption_length - 1, self.vocab_size)).to(device, non_blocking=True)\n",
    "        # Embedding the captions\n",
    "        embeddings = self.embed(captions.int())\n",
    "        # Concat Embeddings with features\n",
    "        embeddings = torch.cat((features.expand((-1, embeddings.shape[1], -1)), embeddings), dim = -1) #shape = (batch_size, 95, 768+512=1280)\n",
    "        # Pass the caption word by word in train mode\n",
    "        if mode == 'train':\n",
    "            #embeddings = torch.roll(embeddings, shifts=-1, dims=-1)\n",
    "            for t in range(outputs.size(1)):\n",
    "                hidden_state_1, cell_state_1 = self.lstm_cell_1(embeddings[:, t, :], (hidden_state_1, cell_state_1))\n",
    "                hidden_state_1 = self.dropout(hidden_state_1)\n",
    "                hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                hidden_state_2 = self.dropout(hidden_state_2)\n",
    "                hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                hidden_state_3 = self.dropout(hidden_state_3)\n",
    "                out = self.fc_out(hidden_state_3)\n",
    "                # build the output tensor\n",
    "                outputs[:, t, :] = out\n",
    "        # In test mode, we generate until length = max_caption_length\n",
    "        else:\n",
    "            t = 0\n",
    "            while t < max_caption_length - 1:\n",
    "                # First time step - feed <sos> token\n",
    "                if t == 0:\n",
    "                    hidden_state_1, cell_state_1 = self.lstm_cell_1(embeddings[:, 0, :], (hidden_state_1, cell_state_1))\n",
    "                    hidden_state_1 = self.dropout(hidden_state_1)\n",
    "                    hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                    hidden_state_2 = self.dropout(hidden_state_2)\n",
    "                    hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                    hidden_state_3 = self.dropout(hidden_state_3)\n",
    "                else:\n",
    "                    prev_output = outputs[:, t-1, :]\n",
    "                    prev_output = torch.argmax(prev_output, dim=-1)\n",
    "                    prev_output = self.embed(prev_output.int())\n",
    "                    prev_output = torch.cat((features.squeeze(dim=1), prev_output), dim=-1)                    \n",
    "                    hidden_state_1, cell_state_1 = self.lstm_cell_1(prev_output, (hidden_state_1, cell_state_1))\n",
    "                    hidden_state_1 = self.dropout(hidden_state_1)\n",
    "                    hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                    hidden_state_2 = self.dropout(hidden_state_2)\n",
    "                    hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                    hidden_state_3 = self.dropout(hidden_state_3)\n",
    "                out = self.fc_out(hidden_state_3)\n",
    "                outputs[:, t, :] = out\n",
    "                t += 1\n",
    "        return outputs\n",
    "\n",
    "# Define Full Captioning Model Class which has a encoder+decoder\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def forward(self, images, captions, mode='train'):\n",
    "        features = self.encoder(images)\n",
    "        out = self.decoder(features, captions, mode)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4cd0587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize LSTM decoder\n",
    "LSTM_decoder = DecoderRNN(embed_size=512, feature_size=768, hidden_size=512, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212edfb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7988333\n"
     ]
    }
   ],
   "source": [
    "# Check number of parameters\n",
    "pytorch_total_params = sum(p.numel() for p in LSTM_decoder.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421e6199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained encoder(s)\n",
    "attribute_classes = [\n",
    "    6, 5, 4, 3, 5, 3, 3, 3, 5, 8, 3, 3, #Shape Attributes\n",
    "    8, 8, 8, #Fabric Attributes\n",
    "    8, 8, 8 #Color Attributes\n",
    "]\n",
    "\n",
    "backbone = torchvision.models.swin_t()\n",
    "backbone.head = torch.nn.Identity()\n",
    "transformer_encoder = ClassifierModel(backbone, 768)\n",
    "# We load the transformer attribute prediction model which had ~0.9 accuracy\n",
    "transformer_encoder.load_state_dict(\n",
    "    torch.load('./models/transformer_unfreeze_attribute_model.pth')['model_state_dict']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e07b96e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaptionModel(\n",
      "  (encoder): SwinTransformer(\n",
      "    (features): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): Permute()\n",
      "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PatchMerging(\n",
      "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): PatchMerging(\n",
      "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): PatchMerging(\n",
      "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embed): Embedding(109, 512)\n",
      "    (lstm_cell_1): LSTMCell(1280, 512)\n",
      "    (lstm_cell_2): LSTMCell(512, 512)\n",
      "    (lstm_cell_3): LSTMCell(512, 512)\n",
      "    (fc_out): Linear(in_features=512, out_features=109, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Drop Classifier Head and just keep feature extractor (backbone)\n",
    "transformer_encoder = transformer_encoder.backbone\n",
    "# Freeze params\n",
    "for param in transformer_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "transformer_caption_model = CaptionModel(transformer_encoder, LSTM_decoder, vocab)\n",
    "print(transformer_caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620cb2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|█| 602/602 [31:49<00:00,  3.17s/batch, BLEU=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 36.58778645268772 train BLEU: 0.22891239821910858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 val:  22%|▏| 14/63 [00:30<01:43,  2.11s/batch, BLEU=0.199, "
     ]
    }
   ],
   "source": [
    "# Training the LSTM model\n",
    "from utils.train_funcs import fit\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(transformer_caption_model.decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(\n",
    "    transformer_caption_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    epochs,\n",
    "    device,\n",
    "    name='rnn_decoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcaa6c",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Our Swin-LSTM captioning model achieves a BLEU4 of 0.3028 and 0.2020 on the train and validation sets respectively. We find that these are considerably high scores considering that we use the smallest version of Swin Transformer and a 8M parameter LSTM. We print out some of the predictions on samples from the validation set, and find that the model is able to produce high-quality captions from the image features. The model sometimes even produces sentences for fashion attributes which are not in the ground truth caption, but are actually present in the image.\n",
    "\n",
    "We first display screenshots from tensorboard for BLEU4 scores of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "371e25b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./tensorboard_screens/LSTM_train.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"./tensorboard_screens/LSTM_train.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb13a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./tensorboard_screens/LSTM_val.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"./tensorboard_screens/LSTM_val.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7b4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some example captioning\n",
    "images, labels, captions = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6174e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the tank top this woman wears has no sleeves and it is with cotton fabric and graphic patterns . the neckline of the tank top is round . this woman wears a three point shorts , with denim fabric and pure color patterns . there is an accessory on her wrist . \n",
      "******************************\n",
      "Actual caption: \n",
      " the tank shirt this female wears has sleeves cut off , its fabric is cotton , and it has solid color patterns . the tank shirt has a suspenders neckline . this female wears a three point shorts , with denim fabric and pure color patterns . this person has neckwear . there is an accessory on her wrist . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/WOMEN-Tees_Tanks-id_00007961-04_7_additional.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.train_funcs import get_predictions,seq2text\n",
    "\n",
    "images = images.to(device)\n",
    "captions = captions.to(device)\n",
    "start_token = vocab['sos']\n",
    "captions_sos = torch.full((images.shape[0],1), fill_value=start_token).to(device, non_blocking=True)\n",
    "transformer_caption_model.eval()\n",
    "transformer_caption_model.to(device)\n",
    "outputs = transformer_caption_model(images, captions_sos, 'test')\n",
    "preds = get_predictions(outputs, shape=(images.shape[0],outputs.shape[1]), device=device)\n",
    "\n",
    "hypothesis, reference = seq2text(preds[0], captions[0], vocab)\n",
    "val_data = np.load('labels/validation_data.npy', allow_pickle=True)\n",
    "\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[0,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab775d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the t shirt this woman wears has short sleeves and it is with cotton fabric and pure color patterns . the neckline of the t shirt is v shape . \n",
      "******************************\n",
      "Actual caption: \n",
      " her t shirt has short sleeves , cotton fabric and solid color patterns . it has a v shape neckline . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/WOMEN-Tees_Tanks-id_00002540-07_1_front.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis, reference = seq2text(preds[10], captions[10], vocab)\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[10,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3796e38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the upper clothing has short sleeves , cotton fabric and pure color patterns . the neckline of it is v shape . the lower clothing is of long length . the fabric is denim and it has pure color patterns . \n",
      "******************************\n",
      "Actual caption: \n",
      " the guy wears a short sleeve shirt with pure color patterns . the shirt is with cotton fabric . it has a v shape neckline . the trousers the guy wears is of long length . the trousers are with denim fabric and pure color patterns . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/MEN-Tees_Tanks-id_00001774-31_4_full.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis, reference = seq2text(preds[24], captions[24], vocab)\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[24,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aec333",
   "metadata": {},
   "source": [
    "### Training Model with ImageNet Weights Encoder\n",
    "\n",
    "To show that our encoder performs well when trained to predict fashion attributes, we compare it with an encoder loaded with pretrained ImageNet-1K weights. All other training steps are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2c37c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder with ImageNet weights\n",
    "transformer_encoder_imgnet = torchvision.models.swin_t(weights='IMAGENET1K_V1')\n",
    "transformer_encoder_imgnet.head = torch.nn.Identity()\n",
    "# Initialize another LSTM decoder\n",
    "LSTM_decoder_imgnet = DecoderRNN(embed_size=512, feature_size=768, hidden_size=512, vocab_size=len(vocab))\n",
    "# Freeze params in encoder\n",
    "for param in transformer_encoder_imgnet.parameters():\n",
    "    param.requires_grad = False\n",
    "transformer_imgnet_model = CaptionModel(transformer_encoder_imgnet, LSTM_decoder_imgnet, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train:  94%|▉| 564/602 [29:50<02:00,  3.16s/batch, BLEU=0.2"
     ]
    }
   ],
   "source": [
    "# Train the model with ImageNet Swin-T backbone\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(transformer_imgnet_model.decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(\n",
    "    transformer_imgnet_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    epochs,\n",
    "    device,\n",
    "    name='rnn_decoder_imgnet_encoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da1f0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing captioning using the model with ResNet weights in the backbone\n",
    "transformer_imgnet_model.eval()\n",
    "transformer_imgnet_model.to(device)\n",
    "outputs = transformer_imgnet_model(images, captions_sos, 'test')\n",
    "preds = get_predictions(outputs, shape=(images.shape[0],outputs.shape[1]), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4703f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the tank shirt this female wears has no sleeves and its fabric is cotton . the pattern of it is graphic . it has a suspenders neckline . this woman wears a three point pants , with denim fabric and solid color patterns . there is an accessory on her wrist . there is an accessory in his her neck . this woman is wearing a ring on her finger . \n",
      "******************************\n",
      "Actual caption: \n",
      " the tank shirt this female wears has sleeves cut off , its fabric is cotton , and it has solid color patterns . the tank shirt has a suspenders neckline . this female wears a three point shorts , with denim fabric and pure color patterns . this person has neckwear . there is an accessory on her wrist . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/WOMEN-Tees_Tanks-id_00007961-04_7_additional.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis, reference = seq2text(preds[0], captions[0], vocab)\n",
    "\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[0,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3282b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the upper clothing has short sleeves , cotton fabric and solid color patterns . it has a crew neckline . \n",
      "******************************\n",
      "Actual caption: \n",
      " her t shirt has short sleeves , cotton fabric and solid color patterns . it has a v shape neckline . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/WOMEN-Tees_Tanks-id_00002540-07_1_front.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis, reference = seq2text(preds[10], captions[10], vocab)\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[10,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f23dc23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n",
      " the upper clothing has short sleeves , cotton fabric and pure color patterns . it has a round neckline . the lower clothing is of long length . the fabric is cotton and it has pure color patterns . \n",
      "******************************\n",
      "Actual caption: \n",
      " the guy wears a short sleeve shirt with pure color patterns . the shirt is with cotton fabric . it has a v shape neckline . the trousers the guy wears is of long length . the trousers are with denim fabric and pure color patterns . eos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../data/images_224x329/MEN-Tees_Tanks-id_00001774-31_4_full.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis, reference = seq2text(preds[24], captions[24], vocab)\n",
    "print('Predicted caption: \\n', ' '.join(hypothesis).split('eos')[0])\n",
    "print('*'*30)\n",
    "print('Actual caption: \\n', ' '.join(reference))\n",
    "Image(url=\"../data/images_224x329/\"+val_data[24,0], width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53470a9",
   "metadata": {},
   "source": [
    "The model with the resnet-weights encoder achieves a BLEU-4 of 0.18 on the validation set. It is evident that the quality of the captions is not as good as the model with the encoder trained to predict attributes as seen from the above examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61905d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
