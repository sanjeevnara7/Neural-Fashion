{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7a4_ynSQHbV"
   },
   "source": [
    "#### GPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xns_7jojQHO_",
    "outputId": "db5d5e73-b129-4b0f-8c01-a3477e9b3f50"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E95TZeZ0J5c"
   },
   "source": [
    "#### Resize all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DGtevY_0MF1",
    "outputId": "ab2e2828-eca2-456e-fe37-b7c309f1acf4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m img_names \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_224x329\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "\n",
    "img_names = os.listdir('..\\data\\images_224x329')\n",
    "\n",
    "os.makedirs('images_224x329', exist_ok = True)\n",
    "count = 0\n",
    "\n",
    "for img_name in img_names:\n",
    "    count += 1\n",
    "    img_path = os.path.join('images', img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    img = cv2.resize(img, (224, 329), interpolation = cv2.INTER_AREA)\n",
    "    cv2.imwrite(os.path.join('images_224x329', img_name), img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr4Kys98OfLx"
   },
   "source": [
    "#### Torch utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ls2S4QOeJi_Q",
    "outputId": "182f3a96-4b95-43a3-f9bf-cdab8c8e40e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.21.6)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations) (1.7.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (4.6.0.66)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations) (4.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "\n",
    "!pip install -U albumentations\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7ONoRH1POdK"
   },
   "source": [
    "#### DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oaM4OXogJ8nH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "\n",
    "#Define Custom Dataset Class\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, shape_file, fabric_file, pattern_file, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.shape_annotations = pd.read_csv(shape_file, sep=' ')\n",
    "        self.fabric_annotations = pd.read_csv(fabric_file, sep=' ')\n",
    "        self.pattern_annotations = pd.read_csv(pattern_file, sep=' ')\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.shape_annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #start_time = time.time()\n",
    "        path = os.path.join(self.root_dir, self.shape_annotations.iloc[idx, 0]) # idx: row | 0: column(image name)\n",
    "        #image = Image.open(path)\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    #convert image from BGR to RGB format\n",
    "        #print(\"Image read time: \", time.time() - start_time)\n",
    "\n",
    "        #start_time = time.time()\n",
    "\n",
    "        shape_tensor = torch.tensor(self.shape_annotations.iloc[idx, 1:])\n",
    "        fabric_tensor = torch.tensor(self.fabric_annotations.iloc[idx, 1:])\n",
    "        pattern_tensor = torch.tensor(self.pattern_annotations.iloc[idx, 1:])\n",
    "\n",
    "        #print(\"Label read time: \", time.time() - start_time)\n",
    "\n",
    "        y1 = torch.cat((shape_tensor, fabric_tensor, pattern_tensor))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        apply_transform = self.transform_data()   #apply augmentations to the data\n",
    "        image = apply_transform(image = img)['image']\n",
    "        #print(\"Transform time: \", time.time() - start_time)\n",
    "        #print(\"Total time: \", time.time() - start_time)\n",
    "\n",
    "        #Check if the transforms are applied properly or not\n",
    "        # invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "        #                         transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ]),\n",
    "        #                        ])\n",
    "\n",
    "        # unnorm_image_data = invTrans(image).clone().detach().cpu().numpy()\n",
    "        # from google.colab.patches import cv2_imshow\n",
    "        # import numpy as np\n",
    "        # cv2_imshow(cv2.cvtColor(np.round(np.transpose(unnorm_image_data, (1, 2, 0))*255), cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        \n",
    "        return image, y1\n",
    "    \n",
    "    \n",
    "    def transform_data(self):\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            transform_data = A.Compose(\n",
    "              [\n",
    "                  #always resize the image to 329x224\n",
    "                  #A.Resize(height = 329, width= 224, interpolation = cv2.INTER_AREA, p=1),\n",
    "                  A.HorizontalFlip(p=0.4),  \n",
    "                  A.ShiftScaleRotate(shift_limit=0.025, scale_limit=0, rotate_limit=15, p=0.5),\n",
    "                  #A.RandomCrop(height = 224, width = 224, p=1),\n",
    "                  #randomly change brightness, contrast, and saturation of the image 50% of the time\n",
    "                  A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue = 0, p=0.5), \n",
    "                  A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1), \n",
    "                  ToTensorV2(p=1),\n",
    "              ])\n",
    "        else:     #augmentations during validation and testing\n",
    "          transform_data = A.Compose(\n",
    "          [\n",
    "              #always resize the image to 329x224\n",
    "              #A.Resize(height = 329, width = 224, p=1),   \n",
    "              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1),\n",
    "              ToTensorV2(p=1),\n",
    "          ])\n",
    "    \n",
    "        return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lx8L0pQJmxd",
    "outputId": "03248fc1-1459-4cb3-8077-158653bf75a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 329, 224])\n",
      "torch.Size([64, 18])\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from utils.customDataset import FashionDataset\n",
    "\n",
    "shape_file = './labels/shape/shape_anno_all.txt'\n",
    "fabric_file = './labels/texture/fabric_ann.txt'\n",
    "pattern_file = './labels/texture/pattern_ann.txt'\n",
    "\n",
    "# Define Image Augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((329, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "dataset = FashionDataset(shape_file, fabric_file, pattern_file, './images_224x329', transform)\n",
    "\n",
    "# Split into train, val sets\n",
    "train, val = torch.utils.data.random_split(dataset, [40543, 2000])\n",
    "train_loader = DataLoader(dataset = train, batch_size = 64, shuffle = True)#, pin_memory = True, num_workers = 8)\n",
    "val_loader = DataLoader(dataset = val, batch_size = 64, shuffle = False)\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX1sNi1jKVsZ"
   },
   "source": [
    "#### Transformer backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "81abbe783d464fdaa9bc4f6dd1df8930",
      "72baf30e9d9f48289d299cecbd0d0d6e",
      "85fdbfa57f2541dcb6bb98a426e80c02",
      "fb4e78eddd1d4ae989e26832aef55a4e",
      "fbabe6da23944878beec1c4ce1f92ad6",
      "332fed202b0b4dc6b1796c34774943ee",
      "67de3c1cfbae4f1ea0697bf6b0cd4e0a",
      "b598a739ec4f43179547fc32fb3ab2ec",
      "7355724270984a88ae3c9771cb8a275e",
      "37b2367de9904b4fa18ed1d073982d02",
      "0e52df02164646d8be30411b9424bac4"
     ]
    },
    "id": "NXx1YudUJoLU",
    "outputId": "6ed16648-9c69-40b9-e6d1-84cd1d040eca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81abbe783d464fdaa9bc4f6dd1df8930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/108M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "# Initialize swin transformer backbone with ImageNet weights\n",
    "backbone = torchvision.models.swin_t(weights='IMAGENET1K_V1')\n",
    "\n",
    "\n",
    "# Remove classifier head\n",
    "backbone.head = torch.nn.Identity()\n",
    "#layers = list(backbone.children())\n",
    "#backbone = nn.Sequential(*list(backbone.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HF4hp08LJphL",
    "outputId": "f3b704df-c24a-40c5-cf47-bed9b3bfc7b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 82, 56]           4,704\n",
      "           Permute-2           [-1, 82, 56, 96]               0\n",
      "         LayerNorm-3           [-1, 82, 56, 96]             192\n",
      "         LayerNorm-4           [-1, 82, 56, 96]             192\n",
      "ShiftedWindowAttention-5           [-1, 82, 56, 96]               0\n",
      "   StochasticDepth-6           [-1, 82, 56, 96]               0\n",
      "         LayerNorm-7           [-1, 82, 56, 96]             192\n",
      "            Linear-8          [-1, 82, 56, 384]          37,248\n",
      "              GELU-9          [-1, 82, 56, 384]               0\n",
      "          Dropout-10          [-1, 82, 56, 384]               0\n",
      "           Linear-11           [-1, 82, 56, 96]          36,960\n",
      "          Dropout-12           [-1, 82, 56, 96]               0\n",
      "  StochasticDepth-13           [-1, 82, 56, 96]               0\n",
      "SwinTransformerBlock-14           [-1, 82, 56, 96]               0\n",
      "        LayerNorm-15           [-1, 82, 56, 96]             192\n",
      "ShiftedWindowAttention-16           [-1, 82, 56, 96]               0\n",
      "  StochasticDepth-17           [-1, 82, 56, 96]               0\n",
      "        LayerNorm-18           [-1, 82, 56, 96]             192\n",
      "           Linear-19          [-1, 82, 56, 384]          37,248\n",
      "             GELU-20          [-1, 82, 56, 384]               0\n",
      "          Dropout-21          [-1, 82, 56, 384]               0\n",
      "           Linear-22           [-1, 82, 56, 96]          36,960\n",
      "          Dropout-23           [-1, 82, 56, 96]               0\n",
      "  StochasticDepth-24           [-1, 82, 56, 96]               0\n",
      "SwinTransformerBlock-25           [-1, 82, 56, 96]               0\n",
      "        LayerNorm-26          [-1, 41, 28, 384]             768\n",
      "           Linear-27          [-1, 41, 28, 192]          73,728\n",
      "     PatchMerging-28          [-1, 41, 28, 192]               0\n",
      "        LayerNorm-29          [-1, 41, 28, 192]             384\n",
      "ShiftedWindowAttention-30          [-1, 41, 28, 192]               0\n",
      "  StochasticDepth-31          [-1, 41, 28, 192]               0\n",
      "        LayerNorm-32          [-1, 41, 28, 192]             384\n",
      "           Linear-33          [-1, 41, 28, 768]         148,224\n",
      "             GELU-34          [-1, 41, 28, 768]               0\n",
      "          Dropout-35          [-1, 41, 28, 768]               0\n",
      "           Linear-36          [-1, 41, 28, 192]         147,648\n",
      "          Dropout-37          [-1, 41, 28, 192]               0\n",
      "  StochasticDepth-38          [-1, 41, 28, 192]               0\n",
      "SwinTransformerBlock-39          [-1, 41, 28, 192]               0\n",
      "        LayerNorm-40          [-1, 41, 28, 192]             384\n",
      "ShiftedWindowAttention-41          [-1, 41, 28, 192]               0\n",
      "  StochasticDepth-42          [-1, 41, 28, 192]               0\n",
      "        LayerNorm-43          [-1, 41, 28, 192]             384\n",
      "           Linear-44          [-1, 41, 28, 768]         148,224\n",
      "             GELU-45          [-1, 41, 28, 768]               0\n",
      "          Dropout-46          [-1, 41, 28, 768]               0\n",
      "           Linear-47          [-1, 41, 28, 192]         147,648\n",
      "          Dropout-48          [-1, 41, 28, 192]               0\n",
      "  StochasticDepth-49          [-1, 41, 28, 192]               0\n",
      "SwinTransformerBlock-50          [-1, 41, 28, 192]               0\n",
      "        LayerNorm-51          [-1, 21, 14, 768]           1,536\n",
      "           Linear-52          [-1, 21, 14, 384]         294,912\n",
      "     PatchMerging-53          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-54          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-55          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-56          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-57          [-1, 21, 14, 384]             768\n",
      "           Linear-58         [-1, 21, 14, 1536]         591,360\n",
      "             GELU-59         [-1, 21, 14, 1536]               0\n",
      "          Dropout-60         [-1, 21, 14, 1536]               0\n",
      "           Linear-61          [-1, 21, 14, 384]         590,208\n",
      "          Dropout-62          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-63          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-64          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-65          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-66          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-67          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-68          [-1, 21, 14, 384]             768\n",
      "           Linear-69         [-1, 21, 14, 1536]         591,360\n",
      "             GELU-70         [-1, 21, 14, 1536]               0\n",
      "          Dropout-71         [-1, 21, 14, 1536]               0\n",
      "           Linear-72          [-1, 21, 14, 384]         590,208\n",
      "          Dropout-73          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-74          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-75          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-76          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-77          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-78          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-79          [-1, 21, 14, 384]             768\n",
      "           Linear-80         [-1, 21, 14, 1536]         591,360\n",
      "             GELU-81         [-1, 21, 14, 1536]               0\n",
      "          Dropout-82         [-1, 21, 14, 1536]               0\n",
      "           Linear-83          [-1, 21, 14, 384]         590,208\n",
      "          Dropout-84          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-85          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-86          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-87          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-88          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-89          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-90          [-1, 21, 14, 384]             768\n",
      "           Linear-91         [-1, 21, 14, 1536]         591,360\n",
      "             GELU-92         [-1, 21, 14, 1536]               0\n",
      "          Dropout-93         [-1, 21, 14, 1536]               0\n",
      "           Linear-94          [-1, 21, 14, 384]         590,208\n",
      "          Dropout-95          [-1, 21, 14, 384]               0\n",
      "  StochasticDepth-96          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-97          [-1, 21, 14, 384]               0\n",
      "        LayerNorm-98          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-99          [-1, 21, 14, 384]               0\n",
      " StochasticDepth-100          [-1, 21, 14, 384]               0\n",
      "       LayerNorm-101          [-1, 21, 14, 384]             768\n",
      "          Linear-102         [-1, 21, 14, 1536]         591,360\n",
      "            GELU-103         [-1, 21, 14, 1536]               0\n",
      "         Dropout-104         [-1, 21, 14, 1536]               0\n",
      "          Linear-105          [-1, 21, 14, 384]         590,208\n",
      "         Dropout-106          [-1, 21, 14, 384]               0\n",
      " StochasticDepth-107          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-108          [-1, 21, 14, 384]               0\n",
      "       LayerNorm-109          [-1, 21, 14, 384]             768\n",
      "ShiftedWindowAttention-110          [-1, 21, 14, 384]               0\n",
      " StochasticDepth-111          [-1, 21, 14, 384]               0\n",
      "       LayerNorm-112          [-1, 21, 14, 384]             768\n",
      "          Linear-113         [-1, 21, 14, 1536]         591,360\n",
      "            GELU-114         [-1, 21, 14, 1536]               0\n",
      "         Dropout-115         [-1, 21, 14, 1536]               0\n",
      "          Linear-116          [-1, 21, 14, 384]         590,208\n",
      "         Dropout-117          [-1, 21, 14, 384]               0\n",
      " StochasticDepth-118          [-1, 21, 14, 384]               0\n",
      "SwinTransformerBlock-119          [-1, 21, 14, 384]               0\n",
      "       LayerNorm-120          [-1, 11, 7, 1536]           3,072\n",
      "          Linear-121           [-1, 11, 7, 768]       1,179,648\n",
      "    PatchMerging-122           [-1, 11, 7, 768]               0\n",
      "       LayerNorm-123           [-1, 11, 7, 768]           1,536\n",
      "ShiftedWindowAttention-124           [-1, 11, 7, 768]               0\n",
      " StochasticDepth-125           [-1, 11, 7, 768]               0\n",
      "       LayerNorm-126           [-1, 11, 7, 768]           1,536\n",
      "          Linear-127          [-1, 11, 7, 3072]       2,362,368\n",
      "            GELU-128          [-1, 11, 7, 3072]               0\n",
      "         Dropout-129          [-1, 11, 7, 3072]               0\n",
      "          Linear-130           [-1, 11, 7, 768]       2,360,064\n",
      "         Dropout-131           [-1, 11, 7, 768]               0\n",
      " StochasticDepth-132           [-1, 11, 7, 768]               0\n",
      "SwinTransformerBlock-133           [-1, 11, 7, 768]               0\n",
      "       LayerNorm-134           [-1, 11, 7, 768]           1,536\n",
      "ShiftedWindowAttention-135           [-1, 11, 7, 768]               0\n",
      " StochasticDepth-136           [-1, 11, 7, 768]               0\n",
      "       LayerNorm-137           [-1, 11, 7, 768]           1,536\n",
      "          Linear-138          [-1, 11, 7, 3072]       2,362,368\n",
      "            GELU-139          [-1, 11, 7, 3072]               0\n",
      "         Dropout-140          [-1, 11, 7, 3072]               0\n",
      "          Linear-141           [-1, 11, 7, 768]       2,360,064\n",
      "         Dropout-142           [-1, 11, 7, 768]               0\n",
      " StochasticDepth-143           [-1, 11, 7, 768]               0\n",
      "SwinTransformerBlock-144           [-1, 11, 7, 768]               0\n",
      "       LayerNorm-145           [-1, 11, 7, 768]           1,536\n",
      "AdaptiveAvgPool2d-146            [-1, 768, 1, 1]               0\n",
      "        Identity-147                  [-1, 768]               0\n",
      " SwinTransformer-148                  [-1, 768]               0\n",
      "          Linear-149                    [-1, 6]           4,614\n",
      "          Linear-150                    [-1, 5]           3,845\n",
      "          Linear-151                    [-1, 4]           3,076\n",
      "          Linear-152                    [-1, 3]           2,307\n",
      "          Linear-153                    [-1, 5]           3,845\n",
      "          Linear-154                    [-1, 3]           2,307\n",
      "          Linear-155                    [-1, 3]           2,307\n",
      "          Linear-156                    [-1, 3]           2,307\n",
      "          Linear-157                    [-1, 5]           3,845\n",
      "          Linear-158                    [-1, 8]           6,152\n",
      "          Linear-159                    [-1, 3]           2,307\n",
      "          Linear-160                    [-1, 3]           2,307\n",
      "          Linear-161                    [-1, 8]           6,152\n",
      "          Linear-162                    [-1, 8]           6,152\n",
      "          Linear-163                    [-1, 8]           6,152\n",
      "          Linear-164                    [-1, 8]           6,152\n",
      "          Linear-165                    [-1, 8]           6,152\n",
      "          Linear-166                    [-1, 8]           6,152\n",
      "AttributeClassifier-167  [[-1, 6], [-1, 5], [-1, 4], [-1, 3], [-1, 5], [-1, 3], [-1, 3], [-1, 3], [-1, 5], [-1, 8], [-1, 3], [-1, 3], [-1, 8], [-1, 8], [-1, 8], [-1, 8], [-1, 8], [-1, 8]]               0\n",
      "================================================================\n",
      "Total params: 18,928,323\n",
      "Trainable params: 18,928,323\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.84\n",
      "Forward/backward pass size (MB): 34991654.26\n",
      "Params size (MB): 72.21\n",
      "Estimated Total Size (MB): 34991727.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier with 18 forks (For each of the 18 attribute categories)\n",
    "class AttributeClassifier(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.forks = torch.nn.ModuleList()\n",
    "        attribute_classes = [\n",
    "            6, 5, 4, 3, 5, 3, 3, 3, 5, 8, 3, 3, #Shape Attributes\n",
    "            8, 8, 8, #Fabric Attributes\n",
    "            8, 8, 8 #Color Attributes\n",
    "        ]\n",
    "        \n",
    "        for class_count in attribute_classes:\n",
    "            fork = torch.nn.Linear(in_features=768, out_features=class_count)\n",
    "            self.forks.append(fork)\n",
    "        #self.common_fork = torch.nn.Linear(in_features=768, out_features = 99)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for index,fork in enumerate(self.forks):\n",
    "            out_fork = fork(x) #Classification\n",
    "            out.append(out_fork)\n",
    "        #out = self.common_fork(x)\n",
    "        return out\n",
    "\n",
    "# Model definition\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = AttributeClassifier()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "model = ClassifierModel()\n",
    "model.to(device)\n",
    "summary(model, (3, 329, 224))\n",
    "\n",
    "# Freeze weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze classifier weights\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di5KR-xiKv6f"
   },
   "source": [
    "#### Transformer training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yHXBVc69KC0N"
   },
   "outputs": [],
   "source": [
    "# Will contain utility functions used for training the model(s)\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "import time\n",
    "\n",
    "#Training Function\n",
    "def fit_classifier(model, train_loader, val_loader, optimizer, loss_func, attributes, epochs=10, device='cpu'):\n",
    "    '''\n",
    "    fit() function to train a classifier model.\n",
    "\n",
    "    args:\n",
    "        model - the model to be trained\n",
    "        train_loader - torch.utils.data.Dataloader() for train set\n",
    "        val_loader - torch.utils.data.Dataloader() for val set\n",
    "        optimizer - optimization algorithm for weight updates\n",
    "        criterion - loss function to be used for training\n",
    "    \n",
    "    keyword args:\n",
    "        epochs - Number of training epochs (default=10)\n",
    "        device - the device for training (default='cpu')\n",
    "    \n",
    "    returns: (train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    best_acc = 0.\n",
    "    \n",
    "    #Iterate epochs\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch {}/{}...:'.format(epoch+1, epochs))\n",
    "        epoch_start_time = time.time()\n",
    "        #Each epoch has a training phase and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            data_loader = None\n",
    "            if phase == 'train':\n",
    "                #Set train mode\n",
    "                model.train()\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                #Set Eval mode\n",
    "                model.eval()\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_corrects = torch.tensor([0.]).to(device)\n",
    "            with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "                idx = 0\n",
    "                #Iterate batches\n",
    "                data_loader_iter = iter(data_loader)\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "                # next_batch = data_loader_iter.next() # start loading the first batch\n",
    "                # next_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n",
    "\n",
    "                # for i in range(len(data_loader)):\n",
    "                #     inputs, labels = next_batch \n",
    "                #     if i + 1 != len(data_loader): \n",
    "                #         # start copying data of next batch\n",
    "                #         next_batch = data_loader_iter.next()\n",
    "                #         next_batch = [ _.cuda(non_blocking=True) for _ in next_batch]\n",
    "                #for inputs, labels in data_loader_iter: \n",
    "                for inputs, labels in tepoch:\n",
    "                    idx += 1\n",
    "                    #print(\"Current start time: \", full_start_time)\n",
    "                    tepoch.set_description(f\"Epoch {epoch} {phase}\")\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.long().to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    #Set gradient calc on only for training phase\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        #start_time = time.time()\n",
    "                        #768x99 outputs across all the attributes\n",
    "                        outputs = model(inputs)\n",
    "                        #print(\"process time 1: \", time.time() - start_time)\n",
    "                        #split the outputs to list of 18 separate outputs\n",
    "                        #start_time = time.time()\n",
    "                        #list_outputs = []\n",
    "                        #prev_count = 0\n",
    "                        #for count in attributes:\n",
    "                          #list_outputs.append(outputs[:, prev_count:prev_count+count])\n",
    "                          #prev_count += count\n",
    "                        #print(\"process time 2: \", time.time() - start_time)\n",
    "                        #start_time = time.time()\n",
    "                        loss = classifier_loss(outputs, labels, loss_func, attributes)\n",
    "                        #print(\"process time 3: \", time.time() - start_time)\n",
    "                        #start_time = time.time()\n",
    "                        preds = classifier_preds(outputs, shape=(inputs.shape[0],labels.shape[1]), device=device)\n",
    "                        #print(\"process time 4: \", time.time() - start_time)\n",
    "                        #Do backprop if phase = train\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels)\n",
    "                    tepoch.set_postfix(loss=loss.item() * inputs.size(0))\n",
    "                \n",
    "                # if idx % 50 == 0:\n",
    "                #   print(idx)\n",
    "                #   print(\"50 batches processing time: \", time.time() - batch_start_time)\n",
    "                #   batch_start_time = time.time()\n",
    "                \n",
    "                #print(\"Previous end time: \", time.time())\n",
    "                \n",
    "                epoch_loss = running_loss / len(data_loader.dataset)\n",
    "                epoch_acc = running_corrects.float() / len(data_loader.dataset)\n",
    "                #tepoch.set_postfix(loss=epoch_loss, accuracy=epoch_acc)\n",
    "                #sleep(0.1)\n",
    "                #print('{} loss: {}, {} acc: {}'.format(phase, epoch_loss, phase, epoch_acc))\n",
    "                if phase == 'val':\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "                else:\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "                    train_acc_history.append(epoch_acc)\n",
    "                \n",
    "                #Saving best model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print('-'*20)\n",
    "        epoch_end_time = time.time()\n",
    "    \n",
    "    print('Best val acc: {}'.format(best_acc))\n",
    "    print(f\"Time taken for an epoch: {epoch_end_time - epoch_start_time}\")\n",
    "    return (train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n",
    "\n",
    "#Loss function for classifier\n",
    "def classifier_loss(outputs, targets, loss_func, attributes):\n",
    "    '''\n",
    "    Loss function that calculates cross-entropy over each output and sums it.\n",
    "\n",
    "    args:\n",
    "        outputs - a list of outputs where each output corresponds to a vector of predictions\n",
    "        targets - a tensor of targets where each target corresponds to a class index\n",
    "\n",
    "    '''\n",
    "    loss_out = torch.empty((len(attributes), 1))\n",
    "    for index, output in enumerate(outputs):\n",
    "        loss_out[index] = loss_func(output, targets[:,index])\n",
    "    return torch.sum(loss_out)\n",
    "\n",
    "#Utility method to get predictions\n",
    "def classifier_preds(outputs, shape, device):\n",
    "    '''\n",
    "    Utility function that returns predictions for a list of outputs\n",
    "\n",
    "    args:\n",
    "        outputs - a list of outputs where each output corresponds to a vector of predictions\n",
    "        shape - shape of the predictions to return\n",
    "    '''\n",
    "    preds = torch.empty(size=shape).to(device)\n",
    "    for index, output in enumerate(outputs):\n",
    "        preds[:,index] = torch.argmax(output, dim=1)\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "fojHUscIJsIT",
    "outputId": "6c599d8e-06b3-4e2c-c420-67645227a507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/25...:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train:  24%|██▎       | 149/634 [01:50<05:58,  1.35batch/s, loss=874]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-24920c1b6b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m fit_classifier(fit_classifier(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a252fd99e4a3>\u001b[0m in \u001b[0;36mfit_classifier\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_func, attributes, epochs, device)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m#         next_batch = [ _.cuda(non_blocking=True) for _ in next_batch]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m#for inputs, labels in data_loader_iter:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0;31m#print(\"Current start time: \", full_start_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "#from utils.train_funcs import fit_classifier, classifier_loss\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 1e-3\n",
    "loss_func = classifier_loss\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "attribute_classes = [\n",
    "    6, 5, 4, 3, 5, 3, 3, 3, 5, 8, 3, 3, #Shape Attributes\n",
    "    8, 8, 8, #Fabric Attributes\n",
    "    8, 8, 8 #Color Attributes\n",
    "]\n",
    "\n",
    "fit_classifier(fit_classifier(\n",
    "    model, \n",
    "    train_loader = train_loader, \n",
    "    val_loader = val_loader, \n",
    "    optimizer = optimizer, \n",
    "    loss_func = ce_loss,\n",
    "    attributes = attribute_classes, \n",
    "    epochs = epochs, \n",
    "    device = device\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wmrDdrVGKuoi",
    "a7a4_ynSQHbV",
    "8E95TZeZ0J5c",
    "yr4Kys98OfLx",
    "FX1sNi1jKVsZ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e52df02164646d8be30411b9424bac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "332fed202b0b4dc6b1796c34774943ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37b2367de9904b4fa18ed1d073982d02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67de3c1cfbae4f1ea0697bf6b0cd4e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72baf30e9d9f48289d299cecbd0d0d6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_332fed202b0b4dc6b1796c34774943ee",
      "placeholder": "​",
      "style": "IPY_MODEL_67de3c1cfbae4f1ea0697bf6b0cd4e0a",
      "value": "100%"
     }
    },
    "7355724270984a88ae3c9771cb8a275e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81abbe783d464fdaa9bc4f6dd1df8930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72baf30e9d9f48289d299cecbd0d0d6e",
       "IPY_MODEL_85fdbfa57f2541dcb6bb98a426e80c02",
       "IPY_MODEL_fb4e78eddd1d4ae989e26832aef55a4e"
      ],
      "layout": "IPY_MODEL_fbabe6da23944878beec1c4ce1f92ad6"
     }
    },
    "85fdbfa57f2541dcb6bb98a426e80c02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b598a739ec4f43179547fc32fb3ab2ec",
      "max": 113445839,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7355724270984a88ae3c9771cb8a275e",
      "value": 113445839
     }
    },
    "b598a739ec4f43179547fc32fb3ab2ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb4e78eddd1d4ae989e26832aef55a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37b2367de9904b4fa18ed1d073982d02",
      "placeholder": "​",
      "style": "IPY_MODEL_0e52df02164646d8be30411b9424bac4",
      "value": " 108M/108M [00:04&lt;00:00, 26.1MB/s]"
     }
    },
    "fbabe6da23944878beec1c4ce1f92ad6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
