{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836b6d14",
   "metadata": {},
   "source": [
    "# Captioning Model\n",
    "\n",
    "This notebook will contain the code to train the captioning models (decoders). We load the encoders that were pre-trained on the 18-attribute data. We compare a LSTM decoder with a (refining) transformer decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df90d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import utils.load_funcs\n",
    "import json\n",
    "import torch,torchvision\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e047de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 329, 224])\n",
      "torch.Size([64, 18])\n",
      "tensor([[15, 13, 48,  ...,  0,  0,  0],\n",
      "        [15,  2, 17,  ...,  0,  0,  0],\n",
      "        [15,  2, 20,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [15,  2, 20,  ...,  0,  0,  0],\n",
      "        [15, 13, 48,  ...,  0,  0,  0],\n",
      "        [15,  2, 20,  ...,  0,  0,  0]]) torch.Size([64, 109])\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_loader, val_loader = utils.load_funcs.get_data_loaders()\n",
    "images, labels, captions = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(captions, captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9300ff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000080-01_7_additional.jpg</th>\n",
       "      <td>The lower clothing is of long length. The fabr...</td>\n",
       "      <td>[15, 2, 78, 44, 3, 34, 35, 54, 1, 2, 5, 3, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-01_7_additional.jpg</th>\n",
       "      <td>His tank top has sleeves cut off, cotton fabri...</td>\n",
       "      <td>[15, 53, 20, 45, 12, 30, 67, 68, 25, 10, 5, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-02_7_additional.jpg</th>\n",
       "      <td>His sweater has long sleeves, cotton fabric an...</td>\n",
       "      <td>[15, 53, 49, 12, 35, 30, 25, 10, 5, 8, 84, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-03_7_additional.jpg</th>\n",
       "      <td>His shirt has short sleeves, cotton fabric and...</td>\n",
       "      <td>[15, 53, 17, 12, 51, 30, 25, 10, 5, 8, 28, 14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEN-Denim-id_00000089-04_7_additional.jpg</th>\n",
       "      <td>The sweater the person wears has long sleeves,...</td>\n",
       "      <td>[15, 2, 49, 2, 26, 9, 12, 35, 30, 25, 47, 5, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           0  \\\n",
       "MEN-Denim-id_00000080-01_7_additional.jpg  The lower clothing is of long length. The fabr...   \n",
       "MEN-Denim-id_00000089-01_7_additional.jpg  His tank top has sleeves cut off, cotton fabri...   \n",
       "MEN-Denim-id_00000089-02_7_additional.jpg  His sweater has long sleeves, cotton fabric an...   \n",
       "MEN-Denim-id_00000089-03_7_additional.jpg  His shirt has short sleeves, cotton fabric and...   \n",
       "MEN-Denim-id_00000089-04_7_additional.jpg  The sweater the person wears has long sleeves,...   \n",
       "\n",
       "                                                                                    sequence  \n",
       "MEN-Denim-id_00000080-01_7_additional.jpg  [15, 2, 78, 44, 3, 34, 35, 54, 1, 2, 5, 3, 10,...  \n",
       "MEN-Denim-id_00000089-01_7_additional.jpg  [15, 53, 20, 45, 12, 30, 67, 68, 25, 10, 5, 8,...  \n",
       "MEN-Denim-id_00000089-02_7_additional.jpg  [15, 53, 49, 12, 35, 30, 25, 10, 5, 8, 84, 7, ...  \n",
       "MEN-Denim-id_00000089-03_7_additional.jpg  [15, 53, 17, 12, 51, 30, 25, 10, 5, 8, 28, 14,...  \n",
       "MEN-Denim-id_00000089-04_7_additional.jpg  [15, 2, 49, 2, 26, 9, 12, 35, 30, 25, 47, 5, 3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaea9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk': 0, '.': 1, 'the': 2, 'is': 3, 'a': 4, 'fabric': 5, 'with': 6, 'patterns': 7, 'and': 8, 'wears': 9, 'cotton': 10, 'her': 11, 'has': 12, 'this': 13, 'color': 14, 'sos': 15, 'eos': 16, 'shirt': 17, 'on': 18, 'there': 19, 'tank': 20, 'it': 21, 'neckline': 22, 'ring': 23, 'an': 24, ',': 25, 'person': 26, 'accessory': 27, 'pure': 28, 'solid': 29, 'sleeves': 30, 'wearing': 31, 'lady': 32, 'female': 33, 'of': 34, 'long': 35, 'pants': 36, 'are': 37, 'three': 38, 'finger': 39, 'wrist': 40, 'graphic': 41, 'point': 42, 'shorts': 43, 'clothing': 44, 'top': 45, 'sleeve': 46, 'its': 47, 'woman': 48, 'sweater': 49, 'denim': 50, 'short': 51, 'in': 52, 'his': 53, 'length': 54, 'crew': 55, 'round': 56, 'chiffon': 57, 't': 58, 'neck': 59, 'neckwear': 60, 'trousers': 61, 'outer': 62, 'hat': 63, 'upper': 64, 'no': 65, 'sleeveless': 66, 'cut': 67, 'off': 68, 'knitting': 69, 'suspenders': 70, 'pattern': 71, 'lapel': 72, 'floral': 73, 'medium': 74, 'v': 75, 'shape': 76, 'head': 77, 'lower': 78, 'other': 79, 'also': 80, 'guy': 81, 'gentleman': 82, 'striped': 83, 'stripe': 84, 'leather': 85, 'belt': 86, 'skirt': 87, 'block': 88, 'complicated': 89, 'mixed': 90, 'leggings': 91, 'waist': 92, 'socks': 93, 'stand': 94, 'man': 95, 'pair': 96, 'plaid': 97, 'lattice': 98, 'sunglasses': 99, 'quarter': 100, 'shoes': 101, 'furry': 102, 'square': 103, 'glasses': 104, 'hands': 105, 'or': 106, 'clothes': 107, 'eyeglasses': 108}\n",
      "Vocab Length:  109\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer and vocab dictionary\n",
    "tokenizer = train_loader.dataset.tokenizer\n",
    "vocab = json.loads(tokenizer.get_config()['index_word'])\n",
    "vocab = {v: int(k)-1 for k, v in vocab.items()}\n",
    "print(vocab)\n",
    "print('Vocab Length: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d65e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Classes for Encoder (Classifier)/Decoder\n",
    "class AttributeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.forks = torch.nn.ModuleList()\n",
    "        for class_count in attribute_classes:\n",
    "            fork = torch.nn.Linear(in_features=in_features, out_features=class_count)\n",
    "            self.forks.append(fork)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for index,fork in enumerate(self.forks):\n",
    "            out_fork = fork(x) #Classification\n",
    "            out.append(out_fork)\n",
    "        return out\n",
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, backbone, backbone_out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = AttributeClassifier(backbone_out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# Define LSTM Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, feature_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "        \n",
    "        # lstm cells\n",
    "        self.lstm_cell_1 = nn.LSTMCell(input_size=embed_size+feature_size, hidden_size=hidden_size)\n",
    "        self.lstm_cell_2 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.lstm_cell_3 = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions, mode='train'):\n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        features = torch.unsqueeze(features, dim=1)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state_1 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_1 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        hidden_state_2 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_2 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        hidden_state_3 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        cell_state_3 = torch.zeros((batch_size, self.hidden_size)).to(device, non_blocking=True)\n",
    "        max_caption_length = 109\n",
    "        \n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.zeros((batch_size, max_caption_length - 1, self.vocab_size)).to(device, non_blocking=True)\n",
    "        # Embedding the captions\n",
    "        embeddings = self.embed(captions.int())\n",
    "        # Concat Embeddings with features\n",
    "        embeddings = torch.cat((features.expand((-1, embeddings.shape[1], -1)), embeddings), dim = -1) #shape = (batch_size, 95, 768+512=1280)\n",
    "        # Pass the caption word by word in train mode\n",
    "        if mode == 'train':\n",
    "            #embeddings = torch.roll(embeddings, shifts=-1, dims=-1)\n",
    "            for t in range(outputs.size(1)):\n",
    "                hidden_state_1, cell_state_1 = self.lstm_cell_1(embeddings[:, t, :], (hidden_state_1, cell_state_1))\n",
    "                hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                out = self.fc_out(hidden_state_3)\n",
    "                # build the output tensor\n",
    "                outputs[:, t, :] = out\n",
    "        # In test mode, we generate until length = max_caption_length\n",
    "        else:\n",
    "            t = 0\n",
    "            while t < max_caption_length:\n",
    "                # First time step - feed <sos> token\n",
    "                if t == 0:\n",
    "                    hidden_state_1, cell_state_1 = self.lstm_cell_1(embeddings[:, 0, :], (hidden_state_1, cell_state_1))\n",
    "                    hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                    hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                else:\n",
    "                    prev_output = outputs[:, t-1, :]\n",
    "                    prev_output = torch.argmax(prev_output, dim=-1)\n",
    "                    prev_output = self.embed(prev_output.int())\n",
    "                    prev_output = torch.cat((features, prev_output), dim=-1)\n",
    "                    \n",
    "                    hidden_state_1, cell_state_1 = self.lstm_cell_1(prev_output, (hidden_state_1, cell_state_1))\n",
    "                    hidden_state_2, cell_state_2 = self.lstm_cell_2(hidden_state_1, (hidden_state_2, cell_state_2))\n",
    "                    hidden_state_3, cell_state_3 = self.lstm_cell_3(hidden_state_2, (hidden_state_3, cell_state_3))\n",
    "                out = self.fc_out(hidden_state_3)\n",
    "                outputs[:, t, :] = out\n",
    "        return outputs\n",
    "\n",
    "# Define Full Captioning Model Class which has a encoder+decoder\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def forward(self, images, captions, mode='train'):\n",
    "        features = self.encoder(images)\n",
    "        if mode != 'train':\n",
    "            start_token = self.vocab['sos']\n",
    "            captions = torch.full((images.shape[0],1), fill_value=start_token)\n",
    "        out = self.decoder(features, captions, mode)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4cd0587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize LSTM decoder\n",
    "LSTM_decoder = DecoderRNN(embed_size=512, feature_size=768, hidden_size=512, vocab_size=len(vocab), num_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212edfb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7988333\n"
     ]
    }
   ],
   "source": [
    "# Check number of parameters\n",
    "pytorch_total_params = sum(p.numel() for p in LSTM_decoder.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421e6199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained encoder(s)\n",
    "attribute_classes = [\n",
    "    6, 5, 4, 3, 5, 3, 3, 3, 5, 8, 3, 3, #Shape Attributes\n",
    "    8, 8, 8, #Fabric Attributes\n",
    "    8, 8, 8 #Color Attributes\n",
    "]\n",
    "\n",
    "backbone = torchvision.models.swin_t()\n",
    "backbone.head = torch.nn.Identity()\n",
    "transformer_encoder = ClassifierModel(backbone, 768)\n",
    "# We load the transformer attribute prediction model which had ~0.9 accuracy\n",
    "transformer_encoder.load_state_dict(\n",
    "    torch.load('./models/transformer_unfreeze_attribute_model.pth')['model_state_dict']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e07b96e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaptionModel(\n",
      "  (encoder): SwinTransformer(\n",
      "    (features): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): Permute()\n",
      "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PatchMerging(\n",
      "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): PatchMerging(\n",
      "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): PatchMerging(\n",
      "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate=none)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embed): Embedding(109, 512)\n",
      "    (lstm_cell_1): LSTMCell(1280, 512)\n",
      "    (lstm_cell_2): LSTMCell(512, 512)\n",
      "    (lstm_cell_3): LSTMCell(512, 512)\n",
      "    (fc_out): Linear(in_features=512, out_features=109, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Drop Classifier Head and just keep feature extractor (backbone)\n",
    "transformer_encoder = transformer_encoder.backbone\n",
    "# Freeze params\n",
    "for param in transformer_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "transformer_caption_model = CaptionModel(transformer_encoder, LSTM_decoder, vocab)\n",
    "print(transformer_caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620cb2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train:  13%|████▌                              | 78/602 [04:14<28:19,  3.24s/batch, BLEU=0.186, loss=39.9]"
     ]
    }
   ],
   "source": [
    "# Training the LSTM model\n",
    "from utils.train_funcs import fit\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(transformer_caption_model.decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(\n",
    "    transformer_caption_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    epochs,\n",
    "    device,\n",
    "    name='rnn_decoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917671b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
